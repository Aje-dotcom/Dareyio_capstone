# SecureLife Insurance Premium Prediction - Model Development & Evaluation
# Advanced Machine Learning Pipeline for Competitive Advantage

# =============================================================================
# 1. LIBRARY IMPORTS & SETUP
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning Libraries
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Regression Models
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Evaluation Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_percentage_error

# Feature Selection
from sklearn.feature_selection import SelectKBest, f_regression, RFE

# Advanced Analytics
import shap
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go

print("üöÄ SECURELIFE INSURANCE - MODEL DEVELOPMENT")
print("=" * 60)
print("üéØ Mission: Build premium prediction model for competitive advantage")
print("üìä Libraries loaded successfully!")

# =============================================================================
# 2. LOAD CLEANED DATA FROM EDA PHASE
# =============================================================================

def load_processed_data():
    """
    Load the cleaned dataset from EDA phase
    """
    # In practice, this would load the cleaned data from the EDA notebook
    # For demonstration, we'll create a realistic dataset
    
    np.random.seed(42)
    n_samples = 1000
    
    # Create realistic insurance data
    data = {
        'age': np.random.randint(18, 65, n_samples),
        'sex': np.random.choice(['Male', 'Female'], n_samples),
        'bmi': np.random.normal(25, 5, n_samples),
        'children': np.random.poisson(1, n_samples),
        'smoker': np.random.choice(['Yes', 'No'], n_samples, p=[0.2, 0.8]),
        'region': np.random.choice(['Northeast', 'Northwest', 'Southeast', 'Southwest'], n_samples),
        'policy_start_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),
        'previous_claims': np.random.poisson(0.5, n_samples),
        'annual_income': np.random.normal(50000, 15000, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # Create target variable with realistic relationships
    premium_base = (
        df['age'] * 50 +
        df['bmi'] * 100 +
        df['children'] * 500 +
        (df['smoker'] == 'Yes') * 10000 +
        df['previous_claims'] * 1000 +
        np.random.normal(0, 1000, n_samples)
    )
    
    df['charges'] = np.maximum(premium_base, 1000)  # Minimum premium
    
    print(f"üìä Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns")
    print(f"üéØ Target variable: charges (Premium Amount)")
    
    return df

# Load the data
df = load_processed_data()

# =============================================================================
# 3. FEATURE ENGINEERING
# =============================================================================

def advanced_feature_engineering(df):
    """
    Create advanced features for insurance premium prediction
    """
    print("\nüîß ADVANCED FEATURE ENGINEERING")
    print("=" * 40)
    
    df_features = df.copy()
    
    # 1. Age-related features
    df_features['age_group'] = pd.cut(df_features['age'], 
                                     bins=[0, 25, 35, 45, 55, 100], 
                                     labels=['Young', 'Adult', 'Middle', 'Senior', 'Elderly'])
    
    # 2. BMI categories
    df_features['bmi_category'] = pd.cut(df_features['bmi'],
                                        bins=[0, 18.5, 25, 30, 100],
                                        labels=['Underweight', 'Normal', 'Overweight', 'Obese'])
    
    # 3. Policy tenure (years since policy start)
    df_features['policy_tenure'] = (pd.Timestamp.now() - df_features['policy_start_date']).dt.days / 365.25
    
    # 4. Risk score combination
    df_features['risk_score'] = (
        (df_features['age'] > 50).astype(int) * 2 +
        (df_features['bmi'] > 30).astype(int) * 3 +
        (df_features['smoker'] == 'Yes').astype(int) * 5 +
        (df_features['previous_claims'] > 0).astype(int) * 2
    )
    
    # 5. Income-to-premium ratio (will be calculated after prediction)
    df_features['high_income'] = (df_features['annual_income'] > df_features['annual_income'].median()).astype(int)
    
    # 6. Family size impact
    df_features['family_size'] = df_features['children'] + 1
    df_features['large_family'] = (df_features['children'] > 2).astype(int)
    
    # 7. Regional risk factors (example)
    region_risk = {'Northeast': 1.1, 'Northwest': 1.0, 'Southeast': 1.2, 'Southwest': 0.9}
    df_features['region_risk_factor'] = df_features['region'].map(region_risk)
    
    print(f"‚úÖ Feature engineering completed!")
    print(f"üìä Features created: {df_features.shape[1] - df.shape[1]} new features")
    
    return df_features

# Apply feature engineering
df_engineered = advanced_feature_engineering(df)

# =============================================================================
# 4. DATA PREPROCESSING PIPELINE
# =============================================================================

def create_preprocessing_pipeline():
    """
    Create comprehensive preprocessing pipeline
    """
    print("\n‚öôÔ∏è CREATING PREPROCESSING PIPELINE")
    print("=" * 40)
    
    # Define feature types
    numerical_features = ['age', 'bmi', 'children', 'policy_tenure', 'risk_score', 
                         'annual_income', 'previous_claims', 'family_size', 'region_risk_factor']
    categorical_features = ['sex', 'smoker', 'region', 'age_group', 'bmi_category']
    
    # Numerical preprocessing
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Categorical preprocessing
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )
    
    print(f"‚úÖ Preprocessing pipeline created")
    print(f"üìä Numerical features: {len(numerical_features)}")
    print(f"üìä Categorical features: {len(categorical_features)}")
    
    return preprocessor, numerical_features, categorical_features

# Create preprocessing pipeline
preprocessor, numerical_features, categorical_features = create_preprocessing_pipeline()

# =============================================================================
# 5. MODEL DEVELOPMENT & COMPARISON
# =============================================================================

def initialize_models():
    """
    Initialize multiple regression models for comparison
    """
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(),
        'Lasso Regression': Lasso(),
        'ElasticNet': ElasticNet(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
        'XGBoost': XGBRegressor(n_estimators=100, random_state=42),
        'LightGBM': LGBMRegressor(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeRegressor(random_state=42),
        'KNN': KNeighborsRegressor(),
        'SVR': SVR()
    }
    
    print(f"ü§ñ Initialized {len(models)} regression models")
    return models

def evaluate_models(X_train, X_test, y_train, y_test, models, preprocessor):
    """
    Evaluate all models and return performance metrics
    """
    print("\nüìä MODEL EVALUATION RESULTS")
    print("=" * 50)
    
    results = {}
    
    for name, model in models.items():
        # Create pipeline
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)
        ])
        
        # Train model
        pipeline.fit(X_train, y_train)
        
        # Make predictions
        y_pred = pipeline.predict(X_test)
        
        # Calculate metrics
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        mape = mean_absolute_percentage_error(y_test, y_pred)
        
        # Cross-validation score
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')
        
        results[name] = {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'R¬≤': r2,
            'MAPE': mape,
            'CV_R¬≤_Mean': cv_scores.mean(),
            'CV_R¬≤_Std': cv_scores.std(),
            'Model': pipeline
        }
        
        print(f"{name:15} | R¬≤: {r2:.4f} | RMSE: {rmse:.2f} | MAE: {mae:.2f}")
    
    return results

# Prepare data for modeling
X = df_engineered.drop(['charges', 'policy_start_date'], axis=1)
y = df_engineered['charges']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"üìä Training set: {X_train.shape[0]} samples")
print(f"üìä Test set: {X_test.shape[0]} samples")

# Initialize and evaluate models
models = initialize_models()
results = evaluate_models(X_train, X_test, y_train, y_test, models, preprocessor)

# =============================================================================
# 6. MODEL SELECTION & HYPERPARAMETER TUNING
# =============================================================================

def select_best_model(results):
    """
    Select best performing model based on R¬≤ score
    """
    best_model_name = max(results.keys(), key=lambda x: results[x]['R¬≤'])
    best_model = results[best_model_name]['Model']
    
    print(f"\nüèÜ BEST MODEL: {best_model_name}")
    print(f"üìä R¬≤ Score: {results[best_model_name]['R¬≤']:.4f}")
    print(f"üìä RMSE: {results[best_model_name]['RMSE']:.2f}")
    
    return best_model_name, best_model

def hyperparameter_tuning(best_model_name, X_train, y_train, preprocessor):
    """
    Perform hyperparameter tuning on the best model
    """
    print(f"\nüîß HYPERPARAMETER TUNING: {best_model_name}")
    print("=" * 40)
    
    # Define parameter grids for different models
    param_grids = {
        'Random Forest': {
            'model__n_estimators': [100, 200, 300],
            'model__max_depth': [10, 20, None],
            'model__min_samples_split': [2, 5, 10],
            'model__min_samples_leaf': [1, 2, 4]
        },
        'XGBoost': {
            'model__n_estimators': [100, 200, 300],
            'model__max_depth': [3, 6, 9],
            'model__learning_rate': [0.01, 0.1, 0.2],
            'model__subsample': [0.8, 0.9, 1.0]
        },
        'LightGBM': {
            'model__n_estimators': [100, 200, 300],
            'model__max_depth': [3, 6, 9],
            'model__learning_rate': [0.01, 0.1, 0.2],
            'model__num_leaves': [31, 50, 100]
        },
        'Gradient Boosting': {
            'model__n_estimators': [100, 200, 300],
            'model__max_depth': [3, 6, 9],
            'model__learning_rate': [0.01, 0.1, 0.2],
            'model__subsample': [0.8, 0.9, 1.0]
        }
    }
    
    if best_model_name in param_grids:
        # Create pipeline
        if best_model_name == 'Random Forest':
            model = RandomForestRegressor(random_state=42)
        elif best_model_name == 'XGBoost':
            model = XGBRegressor(random_state=42)
        elif best_model_name == 'LightGBM':
            model = LGBMRegressor(random_state=42)
        elif best_model_name == 'Gradient Boosting':
            model = GradientBoostingRegressor(random_state=42)
        
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)
        ])
        
        # Perform randomized search
        random_search = RandomizedSearchCV(
            pipeline,
            param_distributions=param_grids[best_model_name],
            n_iter=20,
            cv=5,
            scoring='r2',
            random_state=42,
            n_jobs=-1
        )
        
        random_search.fit(X_train, y_train)
        
        print(f"‚úÖ Best parameters: {random_search.best_params_}")
        print(f"üìä Best CV R¬≤ score: {random_search.best_score_:.4f}")
        
        return random_search.best_estimator_
    
    else:
        print(f"‚ö†Ô∏è No hyperparameter tuning defined for {best_model_name}")
        return None

# Select best model and tune hyperparameters
best_model_name, best_model = select_best_model(results)
tuned_model = hyperparameter_tuning(best_model_name, X_train, y_train, preprocessor)

# =============================================================================
# 7. FINAL MODEL EVALUATION
# =============================================================================

def final_model_evaluation(model, X_test, y_test):
    """
    Comprehensive evaluation of the final tuned model
    """
    print("\nüìä FINAL MODEL EVALUATION")
    print("=" * 40)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    
    print(f"üìà Final Model Performance:")
    print(f"   R¬≤ Score: {r2:.4f}")
    print(f"   RMSE: {rmse:.2f}")
    print(f"   MAE: {mae:.2f}")
    print(f"   MAPE: {mape:.4f}")
    
    # Prediction vs Actual plot
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Premium')
    plt.ylabel('Predicted Premium')
    plt.title('Actual vs Predicted Premium')
    
    plt.subplot(1, 2, 2)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Premium')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'R¬≤': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape,
        'Predictions': y_pred
    }

# Evaluate final model
if tuned_model:
    final_metrics = final_model_evaluation(tuned_model, X_test, y_test)
    final_model = tuned_model
else:
    final_metrics = final_model_evaluation(best_model, X_test, y_test)
    final_model = best_model

# =============================================================================
# 8. FEATURE IMPORTANCE ANALYSIS
# =============================================================================

def analyze_feature_importance(model, X_train, feature_names):
    """
    Analyze feature importance for model interpretability
    """
    print("\nüéØ FEATURE IMPORTANCE ANALYSIS")
    print("=" * 40)
    
    # Get feature importance
    if hasattr(model.named_steps['model'], 'feature_importances_'):
        # For tree-based models
        feature_importance = model.named_steps['model'].feature_importances_
        
        # Get feature names after preprocessing
        preprocessor = model.named_steps['preprocessor']
        feature_names_transformed = preprocessor.get_feature_names_out()
        
        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names_transformed,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print("üìä Top 10 Most Important Features:")
        print(importance_df.head(10))
        
        # Plot feature importance
        plt.figure(figsize=(10, 6))
        sns.barplot(data=importance_df.head(15), x='importance', y='feature')
        plt.title('Feature Importance')
        plt.xlabel('Importance')
        plt.tight_layout()
        plt.show()
        
        return importance_df
    
    else:
        print("‚ö†Ô∏è Model does not support feature importance analysis")
        return None

# Analyze feature importance
feature_importance = analyze_feature_importance(final_model, X_train, X_train.columns)

# =============================================================================
# 9. BUSINESS INSIGHTS & RECOMMENDATIONS
# =============================================================================

def generate_business_recommendations(feature_importance, final_metrics):
    """
    Generate actionable business recommendations for SecureLife Insurance
    """
    print("\nüí° BUSINESS INSIGHTS & RECOMMENDATIONS")
    print("=" * 60)
    
    print("üéØ MODEL PERFORMANCE SUMMARY:")
    print(f"   ‚Ä¢ R¬≤ Score: {final_metrics['R¬≤']:.4f} (Explains {final_metrics['R¬≤']*100:.1f}% of variance)")
    print(f"   ‚Ä¢ RMSE: ${final_metrics['RMSE']:.2f} (Average prediction error)")
    print(f"   ‚Ä¢ MAPE: {final_metrics['MAPE']:.2%} (Mean percentage error)")
    
    print("\nüöÄ COMPETITIVE ADVANTAGES:")
    print("   ‚Ä¢ Data-driven pricing strategy")
    print("   ‚Ä¢ Accurate risk assessment capabilities")
    print("   ‚Ä¢ Optimized premium calculation")
    print("   ‚Ä¢ Reduced underwriting costs")
    
    print("\nüìä KEY PRICING FACTORS:")
    if feature_importance is not None:
        top_features = feature_importance.head(5)['feature'].tolist()
        print("   ‚Ä¢ Focus on top risk factors for pricing")
        print("   ‚Ä¢ Implement dynamic pricing based on model insights")
        print("   ‚Ä¢ Regular model updates with new data")
    
    print("\nüéØ STRATEGIC RECOMMENDATIONS:")
    print("   1. Implement model in production for real-time pricing")
    print("   2. A/B test new pricing strategy vs current approach")
    print("   3. Monitor model performance and retrain quarterly")
    print("   4. Expand model with additional data sources")
    print("   5. Develop specialized models for different customer segments")
    
    print("\nüìà EXPECTED BUSINESS IMPACT:")
    print("   ‚Ä¢ Improved pricing accuracy")
    print("   ‚Ä¢ Reduced adverse selection")
    print("   ‚Ä¢ Enhanced competitive positioning")
    print("   ‚Ä¢ Increased profitability")

# Generate business recommendations
generate_business_recommendations(feature_importance, final_metrics)

# =============================================================================
# 10. MODEL DEPLOYMENT PREPARATION
# =============================================================================

def prepare_model_for_deployment(model, preprocessor, feature_names):
    """
    Prepare model for deployment
    """
    print("\nüöÄ MODEL DEPLOYMENT PREPARATION")
    print("=" * 40)
    
    # Save model components
    import joblib
    
    # Save the final model
    joblib.dump(model, 'securelife_premium_model.pkl')
    print("‚úÖ Model saved as 'securelife_premium_model.pkl'")
    
    # Create prediction function
    def predict_premium(age, sex, bmi, children, smoker, region, 
                       policy_start_date, previous_claims, annual_income):
        """
        Predict insurance premium for new customer
        """
        # Create input DataFrame
        input_data = pd.DataFrame({
            'age': [age],
            'sex': [sex],
            'bmi': [bmi],
            'children': [children],
            'smoker': [smoker],
            'region': [region],
            'policy_start_date': [pd.to_datetime(policy_start_date)],
            'previous_claims': [previous_claims],
            'annual_income': [annual_income]
        })
        
        # Apply same feature engineering
        input_engineered = advanced_feature_engineering(input_data)
        input_features = input_engineered.drop(['charges', 'policy_start_date'], axis=1, errors='ignore')
        
        # Make prediction
        prediction = model.predict(input_features)[0]
        
        return prediction
    
    # Test prediction function
    sample_prediction = predict_premium(
        age=35, sex='Male', bmi=25.0, children=2, smoker='No', 
        region='Northeast', policy_start_date='2023-01-01', 
        previous_claims=0, annual_income=50000
    )
    
    print(f"üß™ Sample prediction: ${sample_prediction:.2f}")
    
    return predict_premium

# Prepare for deployment
predict_premium_func = prepare_model_for_deployment(final_model, preprocessor, X_train.columns)

print("\n‚úÖ MODEL DEVELOPMENT COMPLETED!")
print("üéØ SecureLife Insurance now has a competitive premium prediction model!")
print("üìä Ready for deployment and business impact!")
